[MUSIC] This lecture is about
the Learning to Rank. In this lecture, we are going to
continue talking about web search. In particular we're going to talk
about the using machine learning to combine different features
to improve the ranking function. So the question that we address in
this lecture is how we can combine many features to generate a single ranking
function to optimize search results? In the previous lectures we have talked
about a number of ways to rank documents. We have talked about some retrieval
models like a BM25 or Query Light Code. They can generate a based this course for
matching documents with a query. And we also talked about the link
based approaches like page rank that can give additional scores
to help us improve ranking. Now the question now is,
how can we combine all these features and potentially many other
features to do ranking? And this will be very useful for
ranking webpages, not only just to improve accuracy, but also to improve
the robustness of the ranking function. So that it's not easy for
a spammer to just perturb a one or a few features to promote a page. So the general idea of learning
to rank is to use machine learning to combine this
features to optimize the weights on different features to generate
the optimal ranking function. So we will assume that the given
a query document pair Q and D, we can define a number of features. And these features can vary from
content based features such as a score of the document with
respect to the query according to a retrieval function such as BM25 or
Query Light Hold of punitive commands from a machine or
PL2 etcetera. It can also be a link based score like or
page rank score like. It can be also application of retrieval
models to the ink text of the page. Those are the types of descriptions
of links that point to this page. So, these can all the clues whether
this document is relevant, or not. We can even include a feature
such as whether the URL has a tilde because this might be
indicator of home page or entry page. So all these features can then be combined
together to generate a ranking function. The question is, of course. How can we combine them? In this approach,
we simply hypothesize that the probability that this document isn't relevant to this
query is a function of all these features. So we can hypothesize this that the probability of relevance
is related to these features through a particular form of
the function that has some parameters. These parameters can control the influence of different
features of the final relevance. Now this is of course just an assumption. Whether this assumption really
makes sense is a big question and that's they have to empirically
evaluate the function. But by hypothesizing that
the relevance is related to these features in the particular way, we can
then combine these features to generate the potential more powerful ranking
function, a more robust ranking function. Naturally the next question is how
do we estimate those parameters? How do we know which features
should have a higher weight, and which features will have lower weight? So this is the task of training or
learning, so in this approach what we will
do is use some training data. Those are the data that have
been charted by users so that we already know
the relevant judgments. We already know which documents should
be ranked high for which queries. And this information can be based
on real judgments by users or this can also be approximated by just
using click through information, where we can assume the clicked documents
are better than the skipped documents clicked documents are relevant and
the skipped documents are non-relevant. So in general with the fit
such hypothesize ranking function to the training data meaning that we will try to optimize it's
retrieval accuracy on the training data. And we can adjust these parameters to see how we can optimize the performance of
the functioning on the training data in terms of some measures such as MAP or
NDCG. So the training date would
look like a table of tuples. Each tuple has three elements, the query,
the document, and the judgement. So it looks very much like our
relevance judgement that we talked about in the evaluation
of retrieval systems. [MUSIC]